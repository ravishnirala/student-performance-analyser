# -*- coding: utf-8 -*-
"""student_performance_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NK01MguEab_5vbRDM3LDF4z3OY4fwv3T

*Importing Required Libraries*
"""

import numpy as  np
import pandas as pd
import matplotlib as plt
import seaborn as sns

df=pd.read_csv("/content/drive/MyDrive/archive (1)/StudentsPerformance.csv")

df.head()

df.shape

"""***Preprocessing***



* Check Missing values
* Check Duplicates
* Check data type
* Check the number of unique values in each column
* Check the statistics of the data set
* Check various categories present in the different categorical column







"""

df.isnull().sum()

df.duplicated().sum()

# Checking data type
df.info()

# checking number of unique value
df.nunique()

"""**EDA**"""

print("Categories in 'gender' variable:  ",end=" ")
print(df["gender"].unique())

print("Categories in 'race/ethnicity' variable:  ",end=" ")
print(df["race/ethnicity"].unique())

print("Categories in 'parental level of education' variable:  ",end=" ")
print(df["parental level of education"].unique())

print("Categories in 'lunch' variable:  ",end=" ")
print(df["lunch"].unique())

print("Categories in 'test preparation course' variable:  ",end=" ")
print(df["test preparation course"].unique())

sns.countplot(x='gender', data=df, palette='bright')
#Add labels
plt.xlabel('Gender')
plt.ylabel('Count')
plt.title('Distribution of Gender')
#Display the plot
plt.show()

df['gender'].value_counts()

y=df['race/ethnicity'].value_counts()
sns.barplot(y)
plt.show()
print(y)

y=df['parental level of education'].value_counts()
f=plt.figure(figsize=(15,8))
sns.barplot(y)
plt.show()
print(y)

y=df['lunch'].value_counts()
sns.barplot(y)
plt.show()
print(y)

y=df['test preparation course'].value_counts()
sns.barplot(y)
plt.show()
print(y)

"""***Maximum score in all three subject***"""

# Find the maximum score for each subject
max_math_score = df['math score'].max()
max_reading_score = df['reading score'].max()
max_writing_score = df['writing score'].max()

# Print the maximum scores
print(f"Maximum Math Score: {max_math_score}")
print(f"Maximum Reading Score: {max_reading_score}")
print(f"Maximum Writing Score: {max_writing_score}")

"""***Multivariant analysis among all***"""

# Set figure size
plt.rcParams['figure.figsize'] = (12, 9)

# First row of pie charts
plt.subplot(2, 3, 1)
size = df['gender'].value_counts()
labels = 'Female', 'Male'
color = ['red','green']
plt.pie(size, colors=color, labels=labels, autopct='%.2f%%')
plt.title('Gender', fontsize=20)
plt.axis('off')

plt.subplot(2, 3, 2)
size = df['race/ethnicity'].value_counts()
labels = 'Group C', 'Group D', 'Group B', 'Group E', 'Group A'
color = ['red', 'green', 'blue', 'cyan', 'orange']
plt.pie(size, colors=color, labels=labels, autopct='%.2f%%')
plt.title('Race/Ethnicity', fontsize=20)
plt.axis('off')

plt.subplot(2, 3, 3)
size = df['lunch'].value_counts()
labels = 'Standard', 'Free'
color = ['red', 'green']
plt.pie(size, colors=color, labels=labels, autopct='%.2f%%')
plt.title('Lunch', fontsize=20)
plt.axis('off')

# Second row of pie charts
plt.subplot(2, 3, 4)
size = df['test preparation course'].value_counts()
labels = 'None', 'Completed'
color = ['red', 'green']
plt.pie(size, colors=color, labels=labels, autopct='%.2f%%')
plt.title('Test Course', fontsize=20)
plt.axis('off')

plt.subplot(2, 3, 5)
size = df['parental level of education'].value_counts()
labels = 'Some College', "Associate's Degree", 'High School', 'Some High School', "Bachelor's Degree", "Master's Degree"
color = ['red', 'green', 'blue', 'cyan', 'orange', 'grey']
plt.pie(size, colors=color, labels=labels, autopct='%.2f%%')
plt.title('Parental Education', fontsize=20)
plt.axis('off')

"""***Data preprocessing***



"""

# Identify categorical and numerical columns
categorical_cols = df.select_dtypes(include='object').columns
numerical_cols = df.select_dtypes(include=np.number).columns

# Apply one-hot encoding to categorical columns
df_categorical_encoded = pd.get_dummies(df[categorical_cols], drop_first=True)

# Define the target variable
y = df['math score']

# Concatenate encoded categorical features with numerical features (excluding the target variable from numerical features)
X = pd.concat([df_categorical_encoded, df[numerical_cols].drop('math score', axis=1)], axis=1)

# Display the first few rows of the feature set X
display(X.head())

"""***Train-test split***

"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

"""***Model selection and trainig***

"""

from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)

"""***Model evaluation***

"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R2) Score: {r2}")

"""## Summary:

### Data Analysis Key Findings

*   Categorical features were one-hot encoded, and the target variable 'math score' was separated.
*   The data was split into training and testing sets with an 80/20 ratio.
*   A Linear Regression model was selected and trained on the training data.
*   The model achieved a Mean Absolute Error (MAE) of 4.21, a Mean Squared Error (MSE) of 29.10, a Root Mean Squared Error (RMSE) of 5.39, and an R-squared score of 0.88 on the testing data.

### Insights or Next Steps

*   The R-squared score of 0.88 indicates that the model explains a significant portion of the variance in math scores, suggesting good predictive power.
*   Further analysis could involve exploring other regression models or feature engineering to potentially improve performance.

"""